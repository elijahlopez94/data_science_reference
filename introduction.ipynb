{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://medium.datadriveninvestor.com/how-my-computer-copies-a-baby-machine-learning-types-5ffc8add6b31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Reference Guide for All Things Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four Pillars of Data Science:\n",
    "1. Domain Expertise - Relevant knowledge that helps formulate questions, make nuanced decisions, and assists with interpretation of results.\n",
    "2. Math & Stats - Allows investigation of data patterns to determine relationships.\n",
    "3. Computer Science & Engineering - Utilized for data analysis, machine learning, and higher-order predictive techniques.\n",
    "4. Communication - Makes work accessible to nontechnical audiences/stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_crisp-dm.png/new_crisp-dm.png\" width=\"500\">\n",
    "\n",
    "**_CRISP-DM_** is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\n",
    "\n",
    "Let's take a look at the individual steps involved in CRISP-DM.\n",
    "\n",
    "**_Business Understanding:_**  This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization.  Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you to clarify the requirements and make sure that everyone involved understands what the project is and isn't. \n",
    "\n",
    "During this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project. \n",
    "\n",
    "Good questions for this stage include:\n",
    "\n",
    "- Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "- What business problem(s) will this Data Science project solve for the organization?  \n",
    "- What problems are inside the scope of this project?\n",
    "- What problems are outside the scope of this project?\n",
    "- What data sources are available to us?\n",
    "- What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "- Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\n",
    "\n",
    "**_Data Understanding:_**\n",
    "\n",
    "Once we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project.  This step includes both understanding where our data is coming from, as well as the information contained within the data. \n",
    "\n",
    "Consider the following questions when working through this stage:\n",
    "\n",
    "- What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "- Who controls the data sources, and what steps are needed to get access to the data?\n",
    "- What is our target?\n",
    "- What predictors are available to us?\n",
    "- What data types are the predictors we'll be working with?\n",
    "- What is the distribution of our data?\n",
    "- How many observations does our dataset contain? Do we have a lot of data? Only a little? \n",
    "- Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "- How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\n",
    "\n",
    "**_Data Preparation:_**\n",
    "\n",
    "Once we have a strong understanding of our data, we can move onto preparing the data for our modeling steps. \n",
    "\n",
    "During this stage, we'll want to handle the following issues:\n",
    "\n",
    "- Detecting and dealing with missing values\n",
    "- Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    "- Checking for and removing multicollinearity (correlated predictors)\n",
    "- Normalizing our numeric data\n",
    "- Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    "**_Modeling:_**\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task. \n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "- Is this a classification task? A regression task? Something else?\n",
    "- What models will we try?\n",
    "- How do we deal with overfitting?\n",
    "- Do we need to use regularization or not?\n",
    "- What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "- What loss functions will we use?\n",
    "- What threshold of performance do we consider as successful?\n",
    "\n",
    "**_Evaluation:_**\n",
    "\n",
    "During this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem.  Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both _Business Understanding_ and _Deployment_.  As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope.  Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.  \n",
    "\n",
    "Of course, if the results are satisfactory, then we instead move onto deployment!\n",
    "\n",
    "**_Deployment:_**\n",
    "\n",
    "During this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation.  If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights.  For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data Science follows this general pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Exploratory Data Analysis\n",
    "    * Data Cleaning - Look for missing values, duplicates, mismatched data types, and [outliers](https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/)\n",
    "    * Data Visualization - Observe distributions and class balance or imbalance, measure correlations, \n",
    "    * Feature Engineering - Create new features from existing ones (ratios, encoding, splitting datetime, etc.) or transform the data using logarithm or square root.\n",
    "2. Inferential Statistics\n",
    "    * Declare null/alternative hypotheses\n",
    "    * Establish confidence level (i.e. alpha)\n",
    "    * Complete statistical tests\n",
    "    * Interpret results (p-value)\n",
    "3. Modeling\n",
    "    * Machine Learning\n",
    "        * Baseline model comparison & model selection\n",
    "        * Split data into train/test groups\n",
    "        * Data preprocessing (SMOTE, scaling, PCA/LDA) on X_train and X_test groups separately\n",
    "        * Model training\n",
    "        * Hyperparameter tuning (GridSearch)\n",
    "        * Model evaluation\n",
    "    * Deep Learning\n",
    "        * Model design - ANN, CNN, RNN\n",
    "        * Data Preprocessing - Normalize/standardize inputs\n",
    "        * Model training & optimization (SGD, adam)\n",
    "        * Regularization - Boost model performance (batch, dropout, etc.)\n",
    "        * Model evaluation\n",
    "4. Deployment\n",
    "    * Model serialization - Save the model without the need to retrain\n",
    "    * API Devlelopment - Allows other systems to interact with the model\n",
    "    * Containerization - Package model and dependencies together\n",
    "    * Cloud deployment - Ensures scalability\n",
    "    * Integration - Make accessible to non-technical users\n",
    "    * Maintenance - Monitor performance and retrain as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "Identifying Outliers\n",
    "* 1.5 * IQR\n",
    "\n",
    "Handling Outliers:\n",
    "1. Investigate First:\n",
    "    * Understand the cause of the outliers (error, natural variability, or special cases?).\n",
    "2. Use Domain Knowledge:\n",
    "    * Collaborate with subject matter experts to decide whether the outliers are meaningful or spurious.\n",
    "3. Transform Data:\n",
    "    * Use transformations (e.g., log, square root) to reduce the impact of outliers without removing them.\n",
    "4. Use Robust Models:\n",
    "    * Some machine learning algorithms (e.g., decision trees, random forests) are less sensitive to outliers.\n",
    "5. Explore Alternatives:\n",
    "    * Other methods like z-scores, Mahalanobis distance (for multivariate data), or robust statistical techniques might be more appropriate depending on your data and goals.\n",
    "6. Document Decisions:\n",
    "    * Always document how and why outliers were handled to ensure reproducibility and explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
