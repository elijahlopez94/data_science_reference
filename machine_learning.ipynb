{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elija\\anaconda3\\envs\\Cohort_Env\\lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "# Standard Data Science Toolkit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import category_encoders as ce\n",
    "\n",
    "# Machine Learning Preprocessing and Scoring Metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, OneHotEncoder, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, RepeatedKFold, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, mean_squared_error, r2_score, explained_variance_score, roc_curve, auc\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Machine Learning Algorithms\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Natural Language Processing\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder - Changes Target column values to be in range [0, n)\n",
    "# Instantiate the encoder\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer\n",
    "# Instantiate Column Transformer\n",
    "col_transformer = ColumnTransformer(transformers=[\n",
    "    ('ohe', OneHotEncoder(categories='auto', handle_unknown='ignore'), ['category']),\n",
    "    ('name_2', transformer(), ['columns_to_apply_to'])\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagged_tree = BaggingClassifier(DecisionTreeClassifier(criterion='gini', max_depth=5), n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances + Visualization\n",
    "model = ''\n",
    "\n",
    "features = model.feature_importances_\n",
    "\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "# Create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing_step_1', step_1_action),\n",
    "    ('preprocessing_step_2', step_2_action),\n",
    "    ('clf', clf())\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Print accuracy\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex pipeline\n",
    "\n",
    "def preprocess_data_with_pipeline(X):\n",
    "    \n",
    "    ### Encoding categorical data ###\n",
    "    original_features_encoded = ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"), [\"category\"])\n",
    "    ], remainder=\"passthrough\")\n",
    "    \n",
    "    ### Feature engineering ###\n",
    "    def is_odd(data):\n",
    "        \"\"\"\n",
    "        Helper function that returns 1 if odd, 0 if even\n",
    "        \"\"\"\n",
    "        return data % 2\n",
    "\n",
    "    feature_eng = ColumnTransformer(transformers=[\n",
    "        (\"add_number_odd\", FunctionTransformer(is_odd), [\"number\"])\n",
    "    ], remainder=\"drop\")\n",
    "  \n",
    "    ### Combine encoded and engineered features ###\n",
    "    feature_union = FeatureUnion(transformer_list=[\n",
    "        (\"encoded_features\", original_features_encoded),\n",
    "        (\"engineered_features\", feature_eng)\n",
    "    ])\n",
    "    \n",
    "    ### Pipeline (including scaling) ###\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"feature_union\", feature_union),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    transformed_data = pipe.fit_transform(X)\n",
    "    \n",
    "    ### Re-apply labels (optional step for readability) ###\n",
    "    encoder = original_features_encoded.named_transformers_[\"ohe\"]\n",
    "    category_labels = encoder.categories_[0]\n",
    "    all_cols = list(category_labels) + [\"number\", \"number_odd\"]\n",
    "    return pd.DataFrame(transformed_data, columns=all_cols, index=X.index), pipe\n",
    "    \n",
    "# Reset value of example_X\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "# Test out our new function\n",
    "result, pipe = preprocess_data_with_pipeline(example_X)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "clf = ''\n",
    "mean_clf_cv = cross_val_score(clf, X_train, y_train).mean()\n",
    "print(f\"Mean Cross Validation Score for Random Forest Classifier: {mean_clf_cv :.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "# Set hyperparamter grid\n",
    "param_grid = {\n",
    "    'hyperparameter_1': ['name_1', 'name_2'],\n",
    "    'hyperparameter_2': [None, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Fit to model\n",
    "gs = GridSearchCV(clf, param_grid, cv=3, return_train_score=True)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate training/testing score. Note train/test time also available\n",
    "gs_train_score = gs.cv_results_['mean_train_score'].mean()\n",
    "gs_test_score = gs.cv_results_['mean_test_score'].mean()\n",
    "gs_best_params = gs.best_params_\n",
    "\n",
    "print(f\"Mean Training Score: {gs_train_score :.2%}\")\n",
    "print(f\"Mean Test Score: {gs_test_score :.2%}\")\n",
    "print(f\"Best Parameter Combination Found During Grid Search: {gs_best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommender Systems\n",
    "# Imports (Similar to sklearn but run different)\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Read in values as Surprise dataset \n",
    "reader = Reader(rating_scale=(0, 5))    #Customize to rating range\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "def clean_string(list):\n",
    "# Clean string of new line characters and punctuation then make lower case    \n",
    "    one_string = ''.join(list).replace('\\n', ' ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    lyrics = one_string.translate(translator)\n",
    "    return lyrics.lower()\n",
    "\n",
    "cleaned_string = clean_string(___)\n",
    "cleaned_string\n",
    "\n",
    "# Tokenize\n",
    "tokenized_string = word_tokenize(cleaned_string)\n",
    "\n",
    "# Vectorize\n",
    "def count_vectorize(tokenized_song):\n",
    "    vector = {}\n",
    "    for word in tokenized_song:\n",
    "        if word in vector.keys():\n",
    "            vector[word] += 1\n",
    "        else:\n",
    "            vector[word] = 1\n",
    "     \n",
    "    return vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cohort_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
